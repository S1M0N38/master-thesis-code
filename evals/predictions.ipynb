{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96eb998d-d9f7-45c1-831c-b42b9ce0b513",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "This notebook computes the prediction metrics, i.e., the images from the test split are passed to the model, and the model predicts the top-k classes.\n",
    "These predictions are used to compute the following metrics:\n",
    "- `error_rate`\n",
    "- `hier_dist_mistake`\n",
    "- `hier_dist`\n",
    "\n",
    "For each model, we have 5 runs (5 different models trained with 5 different random seeds, for a total of 25 models trained on each dataset), which are used to estimate the error bars. The prediction metric values can be organized into tables or visualized on a scatter plot (error_rate vs. hier_dist_mistake).\n",
    "\n",
    "The prediction metrics are computed by the `predictions.py` script on the server, and the results are saved with pickle as pandas dataframes.\n",
    "This notebook reads these dataframes and produces the tables and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f2c03-69f1-4b4b-9e37-3824963de895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import subprocess\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.io.formats.style import Styler\n",
    "\n",
    "import utils\n",
    "\n",
    "# Workaround for Firefox horizontal scroll for Dataframes\n",
    "# https://github.com/jupyterlab/jupyterlab/issues/14625#issuecomment-1722137537\n",
    "from IPython.display import display, HTML, Image, SVG\n",
    "display(HTML(\"<style>.jp-OutputArea-output {display:flex}</style>\"))\n",
    "\n",
    "# Matplotlib theme\n",
    "sns.set_theme(context='paper', style='ticks', palette='colorblind')\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=13)\n",
    "plt.rc('ytick', labelsize=13)\n",
    "plt.rc('axes', labelsize=13)\n",
    "plt.rc('legend',fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53309157-dfc2-498d-b9cc-5008ef8c6be7",
   "metadata": {},
   "source": [
    "*Utils functions to generate highlighted tables in html and tex format*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cdf868-34ca-4e7d-a528-98de62449007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_experiments_names = {\n",
    "    \"xe-onehot\": r\"XE One-hot\",\n",
    "    \"xe-mbm\": r\"XE MBM\",\n",
    "    \"xe-b3p\": r\"XE B3P\",\n",
    "    \"cd-bd\": r\"CD BD\",\n",
    "    \"cd-desc\": r\"CD Desc.\",\n",
    "}\n",
    "tex_metrics_names = {\n",
    "    \"error_rate\": \"Error Rate\",\n",
    "    \"hier_dist_mistake\": \"Hier. Dist. M.\",\n",
    "    \"hier_dist\": \"Hier. Dist.\",\n",
    "}\n",
    "\n",
    "\n",
    "def highlight_predictions(dfs: Styler, axis: int = 0) -> Styler:\n",
    "    \"\"\"\n",
    "    Highlight and format a DataFrame with style for prediction metrics.\n",
    "    For all metrics predictions dataframe lower is better.\n",
    "\n",
    "    Args:\n",
    "        dfs (Styler): The pandas Styler object representing the DataFrame to be styled.\n",
    "        axis (int, optional): The axis along which to apply the styling\n",
    "        (0 for rows, 1 for columns). Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Styler: A Styler object with the specified styling applied.\n",
    "    \"\"\"\n",
    "    dfs = dfs.background_gradient(\"Greens_r\", axis=axis, low=1)\n",
    "    dfs = dfs.highlight_min(props=\"font-weight: bold\", axis=axis)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def table_html(df: pd.DataFrame, axis: int = 0) -> Styler:\n",
    "    \"\"\"Create HTML table with the predictions metrics for each experiment.\n",
    "    This table is used to select the best experiment for each type.\n",
    "    This will not be used in the final paper.\n",
    "    \"\"\"\n",
    "    dfs = df.style.format(precision=3)\n",
    "    dfs = highlight_predictions(dfs, axis=axis)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# Multi-index utils functions\n",
    "\n",
    "def table_tex(\n",
    "    df: pd.DataFrame,\n",
    "    tex_experiments_names: dict[str, str],\n",
    "    tex_metrics_names: dict[str, str],\n",
    ") -> str:\n",
    "    \"\"\"Create Tex table with the predictions metrics mean and std for\n",
    "    each experiment type. This table will be include in the paper.\n",
    "    \"\"\"\n",
    "    global std_index\n",
    "    std_index = 0\n",
    "    df_mean = df.groupby(df_experiments[\"name\"]).mean()\n",
    "    df_std = df.groupby(df_experiments[\"name\"]).std().fillna(0)\n",
    "\n",
    "    # Sort columns according to thesis\n",
    "    # Traspose df scales better to more levels\n",
    "    dfs = df_mean.T[list(tex_experiments_names.keys())].style\n",
    "    df_std = df_std.T[list(tex_experiments_names.keys())]\n",
    "\n",
    "    def fmt(value: float, precision: float):\n",
    "        global std_index\n",
    "        std = df_std.stack().values[std_index]\n",
    "        std_index += 1\n",
    "        return rf\"{value:.{precision}f} \\mdseries ± {std:.{precision}f}\"\n",
    "\n",
    "    dfs = highlight_predictions(dfs, axis=1)\n",
    "    # It's ok to use precision=3 for all predictions metrics\n",
    "    dfs = dfs.format(partial(fmt, precision=3))\n",
    "\n",
    "    # Headers Style\n",
    "    dfs = dfs.hide(names=True, axis=0)\n",
    "    dfs = dfs.hide(names=True, axis=1)\n",
    "    dfs = dfs.format_index(lambda m: tex_metrics_names[m], axis=0, level=1)\n",
    "    dfs = dfs.format_index(lambda m: tex_experiments_names[m], axis=1)\n",
    "\n",
    "    # Convert to TeX\n",
    "    dfs = dfs.to_latex(\n",
    "        hrules=True,\n",
    "        column_format=f\"X r *{{{len(df_mean)}}}{{c}}\",\n",
    "        convert_css=True,\n",
    "        multirow_align=\"c\",\n",
    "        clines=\"skip-last;index\",\n",
    "    )\n",
    "    dfs = dfs.replace(\n",
    "        r\"\\cline{1-2}\",  # clines don't work with colored background\n",
    "        rf\"\\hhline{{{'-' * (len(df_mean) + 2)}}}\",  # so replace with hhline\n",
    "        len(hierarchy) - 2,  # on n occurences replace only the first n - 1\n",
    "        # because the last one it replaced by \"\"\n",
    "    ).replace(r\"\\cline{1-2}\", \"\")\n",
    "    # Exract tabular enviroment.\n",
    "    # I prefer to work with tabular instead of table env for the following reasons:\n",
    "    # - Better control of table position and dimensions\n",
    "    # - Better control of caption position\n",
    "    pattern = r\"\\\\begin\\{tabular\\}(.*?)\\\\end\\{tabular\\}\"\n",
    "    dfs = re.search(pattern, dfs, re.DOTALL)\n",
    "    assert dfs is not None, \"Pattern not found\"\n",
    "    dfs = r\"\\begin{tabularx}{\\linewidth}\" + dfs.group(1) + r\"\\end{tabularx}\"\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# Single-index utils functions\n",
    "\n",
    "def df_to_df_s(df):\n",
    "    df_s = {}\n",
    "    for metric in metrics:\n",
    "        _loc = (slice(None), metric), slice(None)\n",
    "        _df = df.loc[_loc]\n",
    "        _df = _df.reset_index().set_index(\"level\")\n",
    "        _df = _df.rename_axis(\"\", axis=1) # maybe set to metric name\n",
    "        _df = _df.rename_axis(index=\"\") # maybe set to metric name\n",
    "        _df = _df.drop('metric', axis=1)\n",
    "        df_s[metric] = _df\n",
    "    return df_s\n",
    "\n",
    "\n",
    "def table_tex_s(\n",
    "    df: pd.DataFrame,\n",
    "    tex_experiments_names: dict[str, str],\n",
    "    tex_metrics_names: dict[str, str],\n",
    ") -> str:\n",
    "\n",
    "    # aggregate and convert to single-index tables\n",
    "    df_mean = df.groupby(df_experiments[\"name\"]).mean()\n",
    "    df_std = df.groupby(df_experiments[\"name\"]).std().fillna(0)\n",
    "    df_mean_s = df_to_df_s(df_mean.T)\n",
    "    df_std_s = df_to_df_s(df_std.T)\n",
    "\n",
    "    dfs = {}\n",
    "    for metric in metrics:\n",
    "        _df_mean = df_mean_s[metric]\n",
    "        _df_std = df_std_s[metric]\n",
    "\n",
    "        global std_index\n",
    "        std_index = 0\n",
    "    \n",
    "        def fmt(value: float, precision: float):\n",
    "            global std_index\n",
    "            std = _df_std.stack().values[std_index]\n",
    "            std_index += 1\n",
    "            return rf\"{value:.{precision}f} \\mdseries ± {std:.{precision}f}\"\n",
    "     \n",
    "        _dfs = highlight_predictions(_df_mean.style, axis=1)\n",
    "        # It's ok to use precision=3 for all predictions metrics\n",
    "        _dfs = _dfs.format(partial(fmt, precision=3))\n",
    "\n",
    "        # Headers Formatting\n",
    "        level_names = {i: f\"level {i}\" for i in _df_mean.index}\n",
    "        _dfs = _dfs.hide(names=True, axis=0)\n",
    "        _dfs = _dfs.hide(names=True, axis=1)\n",
    "        _dfs = _dfs.format_index(lambda e: tex_experiments_names[e], axis=1)\n",
    "        _dfs = _dfs.format_index(lambda l: level_names[l], axis=0)\n",
    "    \n",
    "        _dfs = _dfs.to_latex(\n",
    "            hrules=True,\n",
    "            column_format=f\"X r *{{{len(_df_mean)}}}{{c}}\",\n",
    "            convert_css=True,\n",
    "            multirow_align=\"c\",\n",
    "            clines=\"skip-last;index\",\n",
    "        )\n",
    "    \n",
    "        _dfs = _dfs.replace(\n",
    "            r\"\\cline{1-2}\",  # clines don't work with colored background\n",
    "            rf\"\\hhline{{{'-' * (len(_df_mean) + 2)}}}\",  # so replace with hhline\n",
    "            len(hierarchy) - 2,  # on n occurences replace only the first n - 1\n",
    "            # because the last one it replaced by \"\"\n",
    "        ).replace(r\"\\cline{1-2}\", \"\")\n",
    "        # Exract tabular enviroment.\n",
    "        # I prefer to work with tabular instead of table env for the following reasons:\n",
    "        # - Better control of table position and dimensions\n",
    "        # - Better control of caption position\n",
    "        pattern = r\"\\\\begin\\{tabular\\}(.*?)\\\\end\\{tabular\\}\"\n",
    "        _dfs = re.search(pattern, _dfs, re.DOTALL)\n",
    "        assert _dfs is not None, \"Pattern not found\"\n",
    "        _dfs = r\"\\begin{tabularx}{\\linewidth}\" + _dfs.group(1) + r\"\\end{tabularx}\"\n",
    "\n",
    "        # add to dict of metric-sty\n",
    "        dfs[metric] = _dfs\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b701e-8ad0-4a22-97b3-49988682c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hists():\n",
    "    # For a single experiment\n",
    "    path_experiments = path_root / \"experiments\" / DATASET\n",
    "    path_experiment = path_experiments / df_experiments.iloc[0].name # hard-coded\n",
    "    path_encoding = path_dataset / \"encodings\" / \"onehot.npy\" # hard-coded\n",
    "    assert path_experiments.exists()\n",
    "    assert path_encoding.exists()\n",
    "    \n",
    "    outputs = np.load(path_experiment / \"results\" / \"outputs.npy\")\n",
    "    targets = np.load(path_experiment / \"results\" / \"targets.npy\")\n",
    "    encodings = np.load(path_encoding)\n",
    "    labels = utils.get_labels(targets, encodings)\n",
    "    predictions = utils.get_predictions(outputs, encodings).argmax(axis=1)\n",
    "    \n",
    "    for level in range(len(hierarchy) - 1):\n",
    "        lca_at_lvl = utils.hierarchy_to_lca(hierarchy[level:])\n",
    "    \n",
    "        wrong_mask = hierarchy[level][predictions] != hierarchy[level][labels]\n",
    "        wrong_predictions_at_0 = predictions[wrong_mask]\n",
    "        wrong_labels_at_0 = labels[wrong_mask]\n",
    "    \n",
    "        weights = lca_at_lvl[wrong_predictions_at_0, wrong_labels_at_0]\n",
    "    \n",
    "        # Metrics\n",
    "        print(\"Error Rate:   \", wrong_mask.sum() / len(wrong_mask))\n",
    "        print(\"Error Rate:   \", weights.sum() / len(weights))\n",
    "        \n",
    "        # Count the occurrences of each value\n",
    "        counts = np.bincount(weights, minlength=len(hierarchy) - level + 1)[1:]\n",
    "        print(\"Counts:       \", counts)\n",
    "    \n",
    "        # Create a bar plot for the occurrences\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "        x = np.arange(1, len(hierarchy) - level + 1)\n",
    "        ax1.bar(x, counts)\n",
    "    \n",
    "        # Create another bar plot for the count * value\n",
    "        values = x * counts\n",
    "        ax1.bar(x, values, alpha=0.5)\n",
    "    \n",
    "        # Set the x-axis ticks and labels\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(x)\n",
    "    \n",
    "        # Add labels and title\n",
    "        ax1.set_xlabel('Distance')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.set_title(f'Level {level}')\n",
    "    \n",
    "        # Pie chart plot\n",
    "        ax2.pie(values, labels=x, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "        ax2.set_title('Contribution')\n",
    "    \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def plot_tree():\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for i, level in enumerate(hierarchy[1:], 1):\n",
    "        pre_level = hierarchy[i-1]\n",
    "        for parent in np.unique(level):\n",
    "            childs = np.unique(pre_level[np.where(level == parent)[0]])\n",
    "            edges = [(f\"{i-1}.{child}\", f\"{i}.{parent}\", ) for child in childs]\n",
    "            # TODO: G.add_weighted_edges_from\n",
    "            G.add_edges_from(edges)\n",
    "    \n",
    "    # Draw the graph\n",
    "    fig, ax = plt.subplots(figsize=(13, len(hierarchy)))\n",
    "    pos = nx.nx_agraph.graphviz_layout(G, prog=\"dot\")\n",
    "    nx.draw(G, pos, node_size=10, arrows=False, font_weight='bold', ax=ax, alpha=0.5)  # Increase node size and add labels\n",
    "    \n",
    "    plt.axis('off')  # Remove axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0e3ea-1da5-46e9-9c6d-28027e76305e",
   "metadata": {},
   "source": [
    "## CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f85ed-dfc2-4030-a6d5-1e951dc6f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"CIFAR100\"\n",
    "\n",
    "path_root = Path(\"..\")\n",
    "path_evals = path_root / \"evals\"\n",
    "path_dataset = path_root / \"datasets\" / \"datasets\" / DATASET\n",
    "path_results = path_evals / DATASET / \"results\"\n",
    "path_results.mkdir(parents=True, exist_ok=True)\n",
    "hierarchy = np.load(path_dataset / \"hierarchy\" / \"hierarchy.npy\")\n",
    "metrics = [\"error_rate\", \"hier_dist_mistake\", \"hier_dist\"]\n",
    "\n",
    "path_experiments = {\n",
    "    \"xe-onehot\": sorted((path_evals / DATASET).glob(\"*_xe-onehot\")),\n",
    "    \"xe-mbm\": sorted((path_evals / DATASET).glob(\"*_xe-mbm-beta5.0\")),\n",
    "    \"xe-b3p\": sorted((path_evals / DATASET).glob(\"*_xe-b3p-beta0.4\")),\n",
    "    \"cd-bd\": sorted((path_evals / DATASET).glob(\"*_cd-barz-denzler\")),\n",
    "    \"cd-desc\": sorted((path_evals / DATASET).glob(\"*_cd-desc-pca-d100\")),\n",
    "}\n",
    "\n",
    "df_experiments = pd.DataFrame(columns=[\"name\"])\n",
    "df_metrics = None\n",
    "for name, paths in path_experiments.items():\n",
    "    assert len(paths) == 5\n",
    "    for path in paths:\n",
    "        df_experiments.loc[path.name, \"name\"] = name\n",
    "        path_predictions = path / \"results\" / \"predictions.pkl\"\n",
    "        df_metrics = pd.concat([df_metrics, pd.read_pickle(path_predictions)])\n",
    "\n",
    "df_mean = df_metrics.groupby(df_experiments[\"name\"]).mean()\n",
    "df_std = df_metrics.groupby(df_experiments[\"name\"]).std().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b03763-5502-4dc5-b838-bf19211a8432",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372efbf1-1798-47da-8d73-92c08a009e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b3733-82a5-43b0-a6c6-c84a152fa50e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6845052-d671-4a0d-97e0-34c51874391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single experiemnt\n",
    "path_experiments = path_root / \"experiments\" / DATASET\n",
    "path_experiment = path_experiments / df_experiments.iloc[0].name # hard=coded\n",
    "path_encoding = path_dataset / \"encodings\" / \"onehot.npy\" # hard-coded\n",
    "assert path_experiments.exists()\n",
    "assert path_encoding.exists()\n",
    "\n",
    "outputs = np.load(path_experiment / \"results\" / \"outputs.npy\")\n",
    "targets = np.load(path_experiment / \"results\" / \"targets.npy\")\n",
    "encodings = np.load(path_encoding)\n",
    "labels = utils.get_labels(targets, encodings)\n",
    "predictions = utils.get_predictions(outputs, encodings)\n",
    "\n",
    "# get top-1 predictions at given lvl of hierearchy\n",
    "k = 1\n",
    "level = 2\n",
    "predictions = hierarchy[level][np.argsort(predictions, axis=1)[:, -k:]]\n",
    "labels =  hierarchy[level][labels.reshape(-1, 1)]\n",
    "assert labels.shape == predictions.shape\n",
    "assert predictions.max() == hierarchy[level].max()\n",
    "assert (num := labels.max()) == hierarchy[level].max()\n",
    "\n",
    "# lca  matrix a specific level\n",
    "\n",
    "indexes = np.unique(hierarchy[level:], return_index=True, axis=0)[1]\n",
    "lca = utils.hierarchy_to_lca(hierarchy[level:])\n",
    "lca\n",
    "\n",
    "# idx to hier sorting\n",
    "idx = np.lexsort(hierarchy)\n",
    "\n",
    "\n",
    "# construct confusion matrix\n",
    "size = labels.max() + 1\n",
    "M = np.zeros((size, size), dtype=int)\n",
    "print(M.shape)\n",
    "for i, j in zip(labels, predictions):\n",
    "    M[i, j] += 1\n",
    "# remove diag values (i.e. correct results) for better viz\n",
    "for i in range(len(M)):\n",
    "    M[i, i] = 0\n",
    "\n",
    "\n",
    "# lca matrix at level lvl\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "axes[0].imshow(M)\n",
    "axes[1].imshow(lca)\n",
    "#axes[2].imshow((M * lca)[idx, :][:, idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e459b6-8530-4bcf-aa3e-daa4e6dc55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single experiemnt\n",
    "path_experiments = path_root / \"experiments\" / DATASET\n",
    "path_experiment = path_experiments / df_experiments.iloc[0].name # hard=coded\n",
    "path_encoding = path_dataset / \"encodings\" / \"onehot.npy\" # hard-coded\n",
    "assert path_experiments.exists()\n",
    "assert path_encoding.exists()\n",
    "\n",
    "\n",
    "for level in range(len(hierarchy)):\n",
    "\n",
    "    print(f\"level {level}\")\n",
    "    outputs = np.load(path_experiment / \"results\" / \"outputs.npy\")\n",
    "    targets = np.load(path_experiment / \"results\" / \"targets.npy\")\n",
    "    encodings = np.load(path_encoding)\n",
    "    labels = utils.get_labels(targets, encodings)\n",
    "    predictions = utils.get_predictions(outputs, encodings)\n",
    "\n",
    "\n",
    "    # get top-1 predictions at given lvl of hierearchy\n",
    "    k = 1\n",
    "    #level = 2\n",
    "    predictions = np.argsort(predictions, axis=1)[:, -k:]\n",
    "    labels =  labels.reshape(-1, 1)\n",
    "    assert labels.shape == predictions.shape\n",
    "    \n",
    "    # lca  matrix a specific level\n",
    "    lca = utils.hierarchy_to_lca(hierarchy[level:])\n",
    "    \n",
    "    # construct confusion matrix at specific level\n",
    "    size = labels.max() + 1\n",
    "    M = np.zeros((size, size), dtype=int)\n",
    "    for i, j in zip(labels, predictions):\n",
    "        if hierarchy[level][i] != hierarchy[level][j]:\n",
    "            M[i, j] += 1\n",
    "    \n",
    "    # idx to hier sorting\n",
    "    idx = np.lexsort(hierarchy)\n",
    "    \n",
    "    # lca matrix at level lvl\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    \n",
    "    axes[0].imshow(M[idx, :][:, idx]**0.001)\n",
    "    axes[1].imshow(lca[idx, :][:, idx])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    W = lca * M\n",
    "    \n",
    "    from collections import Counter\n",
    "    count = Counter(W.ravel())\n",
    "    del count[0]\n",
    "    plt.bar(range(len(count)), count.values())\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    plt.bar(range(len(count)), np.array(list(count.values())) * np.array(list(count.keys())))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eff5ee-0b90-4b49-8a1c-79636883b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfca25-a21a-4ddc-b554-d4e5f998825f",
   "metadata": {},
   "source": [
    "### Hist plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46721f-8635-4ada-9221-f60b3ce73b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94b390-81ee-4394-8d77-c73abbf0d545",
   "metadata": {},
   "source": [
    "### Multi-index single-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea0e4a-68a4-4d93-ae23-e9ae5d7a6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_evals / DATASET / \"results\" / \"predictions.tex\", \"w\") as f:\n",
    "    f.write(table_tex(df_metrics, tex_experiments_names, tex_metrics_names))\n",
    "\n",
    "table_html(df_mean.T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdfe63-ba40-456b-b152-c599986713e7",
   "metadata": {},
   "source": [
    "### Single-index multi-tables\n",
    "\n",
    "- `slice(None)` is equivalent to `:`, but the syntax for multi-index selection only support `slice(None)` inside parentesis.\n",
    "- In the following the suffix `_s` is used to indicate dict of sigle-index tables.\n",
    "- The \"metric dimension\" is then encoded as key-value pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8018a4-5fb3-4a02-a539-48e1bc21518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_s = df_to_df_s(df_mean.T)\n",
    "df_std_s = df_to_df_s(df_std.T)\n",
    "dfs_s = table_tex_s(df_metrics, tex_experiments_names, tex_metrics_names)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    display(table_html(df_mean_s[metric], axis=1))\n",
    "    # display(df_std_s[metric])\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    with open(path_evals / DATASET / \"results\" / f\"predictions_{metric}.tex\", \"w\") as f:\n",
    "        f.write(dfs_s[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b19e0b-e1ac-4655-a7d8-6715abc5e10a",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766f3a1-3663-4261-b4ff-1b78a277d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, tex in tex_experiments_names.items():\n",
    "    ax.errorbar(\n",
    "        df_mean_s[\"error_rate\"][name],\n",
    "        df_mean_s[\"hier_dist_mistake\"][name],\n",
    "        xerr=df_std_s[\"error_rate\"][name],\n",
    "        yerr=df_std_s[\"hier_dist_mistake\"][name],\n",
    "        #fmt='o',\n",
    "        #markersize=2,\n",
    "        capsize=2,\n",
    "        label=tex,\n",
    "        elinewidth=1,\n",
    "        markeredgewidth=1.5,\n",
    "        linestyle='dotted',\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Error Rate', labelpad=15)\n",
    "ax.set_ylabel('Hierarchical Distance Mistake', labelpad=15)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    loc='upper center',\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    bbox_to_anchor=(0.5, 1),\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "#fig.tight_layout()\n",
    "path = (path_evals / DATASET / \"results\" / \"predictions_hier_dist_mistake_error_rate_plot.pdf\")\n",
    "plt.savefig(path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00686029-cb32-4208-93d6-1fd4e971a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, nrows=1, figsize=(2.3 * 4, 2.7))\n",
    "\n",
    "for ax, lvl in zip(axes, range(len(hierarchy))):\n",
    "    for name, tex in tex_experiments_names.items():\n",
    "        ax.errorbar(\n",
    "            df_mean_s[\"error_rate\"][name][lvl],\n",
    "            df_mean_s[\"hier_dist_mistake\"][name][lvl],\n",
    "            xerr=df_std_s[\"error_rate\"][name][lvl],\n",
    "            yerr=df_std_s[\"hier_dist_mistake\"][name][lvl],\n",
    "            fmt='o',\n",
    "            markersize=2,\n",
    "            capsize=2,\n",
    "            label=tex,\n",
    "            #elinewidth=1,\n",
    "            #markeredgewidth=1.5,\n",
    "            #linestyle='dotted',\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f'Level {lvl}', labelpad=15)\n",
    "#ax.set_ylabel('Hierarchical Distance Mistake', labelpad=15)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    loc='upper center',\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    bbox_to_anchor=(0.5, 1.2),\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "path = (path_evals / DATASET / \"results\" / \"predictions_hier_dist_mistake_error_rate_plots.pdf\")\n",
    "plt.savefig(path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be819535-e815-4829-916a-7d9d53994bf9",
   "metadata": {},
   "source": [
    "## iNaturalist19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec30a12-efb7-4574-a81f-2a15dd2417b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"iNaturalist19\"\n",
    "\n",
    "path_root = Path(\"..\")\n",
    "path_evals = path_root / \"evals\"\n",
    "path_dataset = path_root / \"datasets\" / \"datasets\" / DATASET\n",
    "path_results = path_evals / DATASET / \"results\"\n",
    "path_results.mkdir(parents=True, exist_ok=True)\n",
    "hierarchy = np.load(path_dataset / \"hierarchy\" / \"hierarchy.npy\")\n",
    "metrics = [\"error_rate\", \"hier_dist_mistake\", \"hier_dist\"]\n",
    "\n",
    "path_experiments = {\n",
    "    \"xe-onehot\": sorted((path_evals / DATASET).glob(\"*_xe-onehot\")),\n",
    "    \"xe-mbm\": sorted((path_evals / DATASET).glob(\"*_xe-mbm-beta15.0\")),\n",
    "    \"xe-b3p\": sorted((path_evals / DATASET).glob(\"*_xe-b3p-beta0.5\")),\n",
    "    \"cd-bd\": sorted((path_evals / DATASET).glob(\"*_cd-barz-denzler\")),\n",
    "    \"cd-desc\": sorted((path_evals / DATASET).glob(\"*_cd-desc-pca-d300\")),\n",
    "}\n",
    "\n",
    "df_experiments = pd.DataFrame(columns=[\"name\"])\n",
    "df_metrics = None\n",
    "for name, paths in path_experiments.items():\n",
    "    for path in paths:\n",
    "        df_experiments.loc[path.name, \"name\"] = name\n",
    "        path_predictions = path / \"results\" / \"predictions.pkl\"\n",
    "        df_metrics = pd.concat([df_metrics, pd.read_pickle(path_predictions)])\n",
    "\n",
    "\n",
    "df_mean = df_metrics.groupby(df_experiments[\"name\"]).mean()\n",
    "df_std = df_metrics.groupby(df_experiments[\"name\"]).std().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330acd3-c984-4db1-b003-a8ea84292491",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca71ab-8a7d-4178-94a8-d62fa4db619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d8746-f6b1-488a-b1fa-e8110b5b07ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8898cf1-e312-49d8-a5da-e4059eee68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single experiemnt\n",
    "path_experiments = path_root / \"experiments\" / DATASET\n",
    "path_experiment = path_experiments / df_experiments.iloc[0].name # hard=coded\n",
    "path_encoding = path_dataset / \"encodings\" / \"onehot.npy\" # hard-coded\n",
    "assert path_experiments.exists()\n",
    "assert path_encoding.exists()\n",
    "\n",
    "\n",
    "for level in range(len(hierarchy)):\n",
    "\n",
    "    print(f\"level {level}\")\n",
    "    outputs = np.load(path_experiment / \"results\" / \"outputs.npy\")\n",
    "    targets = np.load(path_experiment / \"results\" / \"targets.npy\")\n",
    "    encodings = np.load(path_encoding)\n",
    "    labels = utils.get_labels(targets, encodings)\n",
    "    predictions = utils.get_predictions(outputs, encodings)\n",
    "\n",
    "\n",
    "    # get top-1 predictions at given lvl of hierearchy\n",
    "    k = 1\n",
    "    #level = 2\n",
    "    predictions = np.argsort(predictions, axis=1)[:, -k:]\n",
    "    labels =  labels.reshape(-1, 1)\n",
    "    assert labels.shape == predictions.shape\n",
    "    \n",
    "    # lca  matrix a specific level\n",
    "    lca = utils.hierarchy_to_lca(hierarchy[level:])\n",
    "    \n",
    "    # construct confusion matrix at specific level\n",
    "    size = labels.max() + 1\n",
    "    M = np.zeros((size, size), dtype=int)\n",
    "    for i, j in zip(labels, predictions):\n",
    "        if hierarchy[level][i] != hierarchy[level][j]:\n",
    "            M[i, j] += 1\n",
    "    \n",
    "    # idx to hier sorting\n",
    "    idx = np.lexsort(hierarchy)\n",
    "    \n",
    "    # lca matrix at level lvl\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    \n",
    "    axes[0].imshow(M[idx, :][:, idx]**0.5)\n",
    "    axes[1].imshow(lca[idx, :][:, idx])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    W = lca * M\n",
    "    \n",
    "    from collections import Counter\n",
    "    count = Counter(W.ravel())\n",
    "    del count[0]\n",
    "    plt.bar(range(len(count)), count.values())\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a719c-c9fc-4407-859e-e0ef22d96633",
   "metadata": {},
   "source": [
    "### Hist plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a831f6-c579-4c9d-b93d-fa1c3e2348f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd334afd-9a4b-40c2-9210-d1bd63f49c6d",
   "metadata": {},
   "source": [
    "### Multi-index single-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c984b0-8fe9-4897-bd55-196c2393b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_evals / DATASET / \"results\" / \"predictions.tex\", \"w\") as f:\n",
    "    f.write(table_tex(df_metrics, tex_experiments_names, tex_metrics_names))\n",
    "\n",
    "table_html(df_mean.T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965a19d-e8ad-4444-8060-72f7fbef54e3",
   "metadata": {},
   "source": [
    "### Single-index multi-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515eeb7-0806-463f-9bf8-0ef884a9064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_s = df_to_df_s(df_mean.T)\n",
    "df_std_s = df_to_df_s(df_std.T)\n",
    "dfs_s = table_tex_s(df_metrics, tex_experiments_names, tex_metrics_names)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    display(table_html(df_mean_s[metric], axis=1))\n",
    "    # display(df_std_s[metric])\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    with open(path_evals / DATASET / \"results\" / f\"predictions_{metric}.tex\", \"w\") as f:\n",
    "        f.write(dfs_s[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b63c1-3ba2-458e-ad63-0b95526beffd",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026b0a8-740c-4020-86b7-d9f13d1ffdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, tex in tex_experiments_names.items():\n",
    "    ax.errorbar(\n",
    "        df_mean_s[\"error_rate\"][name],\n",
    "        df_mean_s[\"hier_dist_mistake\"][name],\n",
    "        xerr=df_std_s[\"error_rate\"][name],\n",
    "        yerr=df_std_s[\"hier_dist_mistake\"][name],\n",
    "        #fmt='o',\n",
    "        #markersize=2,\n",
    "        capsize=2,\n",
    "        label=tex,\n",
    "        elinewidth=1,\n",
    "        markeredgewidth=1.5,\n",
    "        linestyle='dotted',\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Error Rate', labelpad=15)\n",
    "ax.set_ylabel('Hierarchical Distance Mistake', labelpad=15)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    loc='upper center',\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    bbox_to_anchor=(0.5, 1),\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "#fig.tight_layout()\n",
    "path = (path_evals / DATASET / \"results\" / \"predictions_hier_dist_mistake_error_rate_plot.pdf\")\n",
    "plt.savefig(path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90ae6a-8b1d-4773-9910-25ee5f03f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(2.3 * 3, 2.3 * 2 + 0.2))\n",
    "\n",
    "for ax, lvl in zip(axes.ravel(), range(len(hierarchy))):\n",
    "    for name, tex in tex_experiments_names.items():\n",
    "        ax.errorbar(\n",
    "            df_mean_s[\"error_rate\"][name][lvl],\n",
    "            df_mean_s[\"hier_dist_mistake\"][name][lvl],\n",
    "            xerr=df_std_s[\"error_rate\"][name][lvl],\n",
    "            yerr=df_std_s[\"hier_dist_mistake\"][name][lvl],\n",
    "            fmt='o',\n",
    "            markersize=2,\n",
    "            capsize=2,\n",
    "            label=tex,\n",
    "            #elinewidth=1,\n",
    "            #markeredgewidth=1.5,\n",
    "            #linestyle='dotted',\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f'Level {lvl}', labelpad=15)\n",
    "#ax.set_ylabel('Hierarchical Distance Mistake', labelpad=15)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    loc='upper center',\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    bbox_to_anchor=(0.5, 1.1),\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "path = (path_evals / DATASET / \"results\" / \"predictions_hier_dist_mistake_error_rate_plots.pdf\")\n",
    "plt.savefig(path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c31099-f80a-4a10-813b-cf21c2bd0cdb",
   "metadata": {},
   "source": [
    "## TieredImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5afb77-b2c7-48fd-89ca-8dddcae79b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"tieredImageNet\"\n",
    "\n",
    "path_root = Path(\"..\")\n",
    "path_evals = path_root / \"evals\"\n",
    "path_dataset = path_root / \"datasets\" / \"datasets\" / DATASET\n",
    "path_results = path_evals / DATASET / \"results\"\n",
    "path_results.mkdir(parents=True, exist_ok=True)\n",
    "hierarchy = np.load(path_dataset / \"hierarchy\" / \"hierarchy.npy\")\n",
    "metrics = [\"error_rate\", \"hier_dist_mistake\", \"hier_dist\"]\n",
    "\n",
    "path_experiments = {\n",
    "    \"xe-onehot\": sorted((path_evals / DATASET).glob(\"*_xe-onehot\")),\n",
    "    \"xe-mbm\": sorted((path_evals / DATASET).glob(\"*_xe-mbm-beta15.0\")),\n",
    "    \"xe-b3p\": sorted((path_evals / DATASET).glob(\"*_xe-b3p-beta0.5\")),\n",
    "    \"cd-bd\": sorted((path_evals / DATASET).glob(\"*_cd-barz-denzler\")),\n",
    "    \"cd-desc\": sorted((path_evals / DATASET).glob(\"*_cd-desc-pca-d300\")),\n",
    "}\n",
    "\n",
    "df_experiments = pd.DataFrame(columns=[\"name\"])\n",
    "df_metrics = None\n",
    "for name, paths in path_experiments.items():\n",
    "    # print(name, len(paths))\n",
    "    for path in paths:\n",
    "        df_experiments.loc[path.name, \"name\"] = name\n",
    "        path_predictions = path / \"results\" / \"predictions.pkl\"\n",
    "        df_metrics = pd.concat([df_metrics, pd.read_pickle(path_predictions)])\n",
    "\n",
    "\n",
    "df_mean = df_metrics.groupby(df_experiments[\"name\"]).mean()\n",
    "df_std = df_metrics.groupby(df_experiments[\"name\"]).std().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a06e98-64c1-4336-a6e2-c338233a66fa",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1bbae-70fb-4098-adcd-fb4ea8ec3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01c9a0-017a-4444-9cba-8d540a82a4b2",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d475c47-cd12-4fd1-aa09-f5a16ec96191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single experiemnt\n",
    "path_experiments = path_root / \"experiments\" / DATASET\n",
    "path_experiment = path_experiments / df_experiments.iloc[0].name # hard=coded\n",
    "path_encoding = path_dataset / \"encodings\" / \"onehot.npy\" # hard-coded\n",
    "assert path_experiments.exists()\n",
    "assert path_encoding.exists()\n",
    "\n",
    "\n",
    "for level in range(len(hierarchy)):\n",
    "\n",
    "    print(f\"level {level}\")\n",
    "    outputs = np.load(path_experiment / \"results\" / \"outputs.npy\")\n",
    "    targets = np.load(path_experiment / \"results\" / \"targets.npy\")\n",
    "    encodings = np.load(path_encoding)\n",
    "    labels = utils.get_labels(targets, encodings)\n",
    "    predictions = utils.get_predictions(outputs, encodings)\n",
    "\n",
    "\n",
    "    # get top-1 predictions at given lvl of hierearchy\n",
    "    k = 1\n",
    "    #level = 2\n",
    "    predictions = np.argsort(predictions, axis=1)[:, -k:]\n",
    "    labels =  labels.reshape(-1, 1)\n",
    "    assert labels.shape == predictions.shape\n",
    "    \n",
    "    # lca  matrix a specific level\n",
    "    lca = utils.hierarchy_to_lca(hierarchy[level:])\n",
    "    \n",
    "    # construct confusion matrix at specific level\n",
    "    size = labels.max() + 1\n",
    "    M = np.zeros((size, size), dtype=int)\n",
    "    for i, j in zip(labels, predictions):\n",
    "        if hierarchy[level][i] != hierarchy[level][j]:\n",
    "            M[i, j] += 1\n",
    "    \n",
    "    # idx to hier sorting\n",
    "    idx = np.lexsort(hierarchy)\n",
    "    \n",
    "    # lca matrix at level lvl\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    \n",
    "    axes[0].imshow(M[idx, :][:, idx]**0.5)\n",
    "    axes[1].imshow(lca[idx, :][:, idx])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    W = lca * M\n",
    "    \n",
    "    from collections import Counter\n",
    "    count = Counter(W.ravel())\n",
    "    del count[0]\n",
    "    plt.bar(range(len(count)), count.values())\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afd39c-2559-44a1-97b0-d0b81ef4f51a",
   "metadata": {},
   "source": [
    "### Hist plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a8505-1c76-4c20-8f01-03a15d3cabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907f072-899a-49c7-a860-d2cd4da7a279",
   "metadata": {},
   "source": [
    "### Multi-index sigle-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666fc50-b18b-4de0-866c-b44b46d9a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_evals / DATASET / \"results\" / \"predictions.tex\", \"w\") as f:\n",
    "    f.write(table_tex(df_metrics, tex_experiments_names, tex_metrics_names))\n",
    "\n",
    "table_html(df_mean.T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cf634-ce68-455c-ae94-47e12c833142",
   "metadata": {},
   "source": [
    "### Single-index multi-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65389bc-a1c9-4e48-87c6-f8109cdc32ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_s = df_to_df_s(df_mean.T)\n",
    "df_std_s = df_to_df_s(df_std.T)\n",
    "dfs_s = table_tex_s(df_metrics, tex_experiments_names, tex_metrics_names)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    display(table_html(df_mean_s[metric], axis=1))\n",
    "    # display(df_std_s[metric])\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    with open(path_evals / DATASET / \"results\" / f\"predictions_{metric}.tex\", \"w\") as f:\n",
    "        f.write(dfs_s[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f105a-02d3-4ca2-a706-8b87c454ae4f",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187766e-87a6-4fa3-b727-8fc59c03d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, tex in tex_experiments_names.items():\n",
    "    ax.errorbar(\n",
    "        df_mean_s[\"error_rate\"][name],\n",
    "        df_mean_s[\"hier_dist_mistake\"][name],\n",
    "        xerr=df_std_s[\"error_rate\"][name],\n",
    "        yerr=df_std_s[\"hier_dist_mistake\"][name],\n",
    "        #fmt='o',\n",
    "        #markersize=2,\n",
    "        capsize=2,\n",
    "        label=tex,\n",
    "        elinewidth=1,\n",
    "        markeredgewidth=1.5,\n",
    "        linestyle='dotted',\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Error Rate', labelpad=15)\n",
    "ax.set_ylabel('Hierarchical Distance Mistake', labelpad=15)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    loc='upper center',\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    bbox_to_anchor=(0.5, 1),\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "#fig.tight_layout()\n",
    "path = (path_evals / DATASET / \"results\" / \"predictions_hier_dist_mistake_error_rate_plot.pdf\")\n",
    "plt.savefig(path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ef582-b073-49b6-b614-afb1391a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(2.3 * 3, 2.3 * 3 + 0.2))\n",
    "\n",
    "for ax, lvl in zip(axes.ravel(), range(len(hierarchy))):\n",
    "    for name, tex in tex_experiments_names.items():\n",
    "        ax.errorbar(\n",
    "            df_mean_s[\"error_rate\"][name][lvl],\n",
    "            df_mean_s[\"hier_dist_mistake\"][name][lvl],\n",
    "            xerr=df_std_s[\"error_rate\"][name][lvl],\n",
    "            yerr=df_std_s[\"hier_dist_mistake\"][name][lvl],\n",
    "            fmt='o',\n",
    "            markersize=2,\n",
    "            capsize=2,\n",
    "            label=tex,\n",
    "            #elinewidth=1,\n",
    "            #markeredgewidth=1.5,\n",
    "            #linestyle='dotted',\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f'Level {lvl}', labelpad=15)\n",
    "#ax.set_ylabel('Hierarchical Distance Mistake', labelpad=15)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(\n",
    "    loc='upper center',\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    bbox_to_anchor=(0.5, 1.1),\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "path = (path_evals / DATASET / \"results\" / \"predictions_hier_dist_mistake_error_rate_plots.pdf\")\n",
    "plt.savefig(path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
